<!-- markdownlint-disable first-line-h1 -->
<!-- markdownlint-disable html -->
<!-- markdownlint-disable no-duplicate-header -->

<div align="center">
  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-Open-Infra" />
</div>
<hr>

# Hello, DeepSeek Open Infra!

## 202502 Open-Source Week
We're a tiny team @deepseek-ai pushing our limits in AGI exploration.

Starting **this week** , Feb 24, 2025 we'll open-source 5 repos â€“ one daily drop â€“ not because we've made grand claims, 
but simply as developers sharing our small-but-sincere progress with full transparency.

These are humble building blocks of our online service: documented, deployed and battle-tested in production. 
No vaporware, just sincere code that moved our tiny yet ambitious dream forward.

Why? Because every line shared becomes collective momentum that accelerates the journey.
Daily unlocks begin soon. No ivory towers - just pure garage-energy and community-driven innovation ðŸ”§

Stay tuned â€“ let's geek out in the open together.

### Day 1 - [FlashMLA](https://github.com/deepseek-ai/FlashMLA)
**Efficient MLA Decoding Kernel for Hopper GPUs**  
Optimized for variable-length sequences, battle-tested in production  

ðŸ”— <a href="https://github.com/deepseek-ai/FlashMLA"><b>FlashMLA GitHub Repo</b></a>  
âœ… BF16 support  
âœ… Paged KV cache (block size 64)  
âš¡ Performance: 3000 GB/s memory-bound | BF16 580 TFLOPS compute-bound on H800

### Day 2 - [DeepEP](https://github.com/deepseek-ai/DeepEP)

Excited to introduce **DeepEP** - the first open-source EP communication library for MoE model training and inference.

ðŸ”— <a href="https://github.com/deepseek-ai/DeepEP"><b>DeepEP GitHub Repo</b></a>  
âœ… Efficient and optimized all-to-all communication  
âœ… Both intranode and internode support with NVLink and RDMA  
âœ… High-throughput kernels for training and inference prefilling  
âœ… Low-latency kernels for inference decoding  
âœ… Native FP8 dispatch support  
âœ… Flexible GPU resource control for computation-communication overlapping  

### Day 3 - [DeepGEMM](https://github.com/deepseek-ai/DeepGEMM)

Introducing DeepGEMM - an FP8 GEMM library that supports both dense and MoE GEMMs, powering V3/R1 training and inference.

âš¡ Up to 1350+ FP8 TFLOPS on Hopper GPUs
âœ… No heavy dependency, as clean as a tutorial
âœ… Fully Just-In-Time compiled
âœ… Core logic at ~300 lines - yet outperforms expert-tuned kernels across most matrix sizes
âœ… Supports dense layout and two MoE layouts

ðŸ”— GitHub: https://github.com/deepseek-ai/DeepGEMM



### Ongoing Releases...

## 2024 AI Infrastructure Paper (SC24)   
### Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning

<a href="https://dl.acm.org/doi/10.1109/SC41406.2024.00089"><b>ðŸ“„ Paper Link</b></a>
<a href="https://arxiv.org/abs/2408.14158"><b>ðŸ“„ Arxiv Paper Link</b></a>
